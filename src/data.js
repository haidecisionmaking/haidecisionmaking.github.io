const data = [


  {paper:'Human Decision Making with Machine Assistance: An Experiment on Bailing and Jailing', paper_link:'', year:2019, venue:'CSCW', authors:'Nina Grgic-Hlaca, Christoph Engel, Krishna P. Gummadi', ai_model:'', ai_assistance_element:'Prediction (binary prediction)', actual_task:'Recidivism prediction', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'Trust and reliance [objective] (switch)', key:'grgic2019human'},
  {paper:'Progressive Disclosure: Designing for Effective Transparency', paper_link:'https://dl.acm.org/doi/abs/10.1145/3301275.3302322', year:2019, venue:'IUI', authors:'Aaron Springer, Steve Whittaker', ai_model:'Generalized additive models (GAMs)', ai_assistance_element:'Prediction (continuous prediction or regression)', actual_task:'Emotion analysis', ai_task_type:'Discovery', task_eval_metric:'Task satisfaction & mental demand [subjective] (workload)', ai_eval_metric:'Trust and reliance [subjective] (self-reported trust, perceived accuracy); System satisfaction and usability [subjective] (system affect)', key:'springer2018progressive'},
  {paper:'COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations', paper_link:'https://dl.acm.org/doi/abs/10.1145/3313831.3376615', year:2020, venue:'CHI', authors:'Ashraf Abdul,  Christian von der Weth,  Mohan Kankanhalli,  Brian Y. Lim', ai_model:'Generalized additive models (GAMs)', ai_assistance_element:'Prediction (continuous prediction or regression); Global feature importance (shape function of GAMs)', actual_task:'Property price prediction', ai_task_type:'Discovery', task_eval_metric:'Efficiency [objective] (time taken on the task)', ai_eval_metric:'Understanding [objective] (forward simulation); Trust and reliance [subjective] (self-reported trust, usage intention/willingness); Others [subjective] (explanation workload)', key:'abdul2020cogam'},
  {paper:'Why and Why Not Explanations Improve the Intelligibility of Context-Aware Intelligent Systems', paper_link:'https://dl.acm.org/doi/10.1145/1518701.1519023', year:2009, venue:'CHI', authors:'Brian Y. Lim, Anind K. Dey, Daniel Avrahami', ai_model:'Decision trees/random forests', ai_assistance_element:'Prediction (binary prediction); Rule-based explanations (tree-based explanation); Counterfactual explanations (counterfactual examples)', actual_task:'Activity recognition', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error); Efficiency [objective] (time taken on the task)', ai_eval_metric:'', key:'lim2009and'},
  {paper:'Investigating User Confidence for Uncertainty Presentation in Predictive Decision Making', paper_link:'https://dl.acm.org/doi/abs/10.1145/2838739.2838753', year:2015, venue:'OzCHI', authors:'Syed Arshad, Jianlong Zhou, Constant Bridon, Fang Chen, Yang Wang', ai_model:'', ai_assistance_element:'Prediction (continuous prediction or regression, prediction with alternates); Model uncertainty (classification confidence (or probability))', actual_task:'Water pipe failure prediction', ai_task_type:'Discovery', task_eval_metric:'Efficiency [objective] (time taken on the task); Task satisfaction & mental demand [subjective] (task difficulty)', ai_eval_metric:'', key:'arshad2015investigating'},
  {paper:'Will you Accept an Imperfect AI? Exploring Designs for Adjusting End-user Expectations of AI Systems', paper_link:'http://library.usc.edu.ph/ACM/CHI2019/1proc/paper411.pdf', year:2019, venue:'CHI', authors:'Rafal Kocielnik, Saleema Amershi, Paul N. Bennet', ai_model:'', ai_assistance_element:'No prediction shown (no prediction shown); Global example-based explanations (prototypes); Model documentation (overview of the model or algorithm); Levels of user agency (outcome control (before decision))', actual_task:'Meeting scheduling assistance', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error); Efficiency [objective] (time taken on the task)', ai_eval_metric:'Trust and reliance [subjective] (perceived accuracy, usage intention/willingness); System satisfaction and usability [subjective] (satisfaction, helpfulness/support, system frustration, recommendation to others)', key:'kocielnik2019will'},
  {paper:'Do Explanations make VQA Models more Predictable to a Human?', paper_link:'https://www.aclweb.org/anthology/D18-1128/', year:2018, venue:'ACL', authors:'Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, Devi Parikh', ai_model:'Convolution Neural Networks', ai_assistance_element:'No prediction shown (no prediction shown); Local feature importance (attention, gradient-based); Interventions or workflows affecting cognitive process (outcome feedback to user, training phase)', actual_task:'Question answering', ai_task_type:'Emulation', task_eval_metric:'', ai_eval_metric:'Understanding [objective] (forward simulation); Trust and reliance [subjective] (self-reported agreement/reliance)', key:'chandrasekaran2018explanations'},
  {paper:'I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI', paper_link:'https://dl.acm.org/doi/10.1145/3397481.3450644', year:2021, venue:'IUI', authors:'Michael Chromik, Malin Eiband, Felicitas Buchner, Adrian Krüger, Andreas Butz', ai_model:'Decision trees/random forests', ai_assistance_element:'Prediction (continuous prediction or regression); Local feature importance (perturbation-based SHAP)', actual_task:'Loan risk prediction', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [objective] (forward simulation, correctness of described model behaviors); Trust and reliance [subjective] (self-reported trust, model confidence/acceptance, usage intention/willingness)', key:'chromik2021think'},
  {paper:'Explainable Active Learning (XAL): An Empirical Study of How Local Explanations Impact Annotator Experience', paper_link:'https://arxiv.org/abs/2001.09219', year:2021, venue:'CSCW', authors:'Bhavya Ghai, Q. Vera Liao, Yunfeng Zhang, Rachel Bellamy, Klaus Mueller', ai_model:'Generalized additive models (GAMs)', ai_assistance_element:'', actual_task:'', ai_task_type:'', task_eval_metric:'', ai_eval_metric:'', key:'ghai2020explainable'},
  {paper:'The accuracy, fairness, and limits of predicting recidivism', paper_link:'https://advances.sciencemag.org/content/4/1/eaao5580/tab-pdf', year:2018, venue:'Science Advances', authors:'Julia Dressel, Hany Farid', ai_model:'Logistic regression; Support-vector machines (SVMs)', ai_assistance_element:'', actual_task:'', ai_task_type:'', task_eval_metric:'Efficacy [objective] (AUC-ROC, false negative rate, true positive rate, true negative rate)', ai_eval_metric:'', key:'dressel2018accuracy'},
  {paper:'Comparing Automatic and Human Evaluation of Local Explanations for Text Classification', paper_link:'https://www.aclweb.org/anthology/N18-1097.pdf', year:2018, venue:'ACL', authors:'Dong Nguyen', ai_model:'Logistic regression; Shallow (1- to 2-layer) neural networks', ai_assistance_element:'No prediction shown (no prediction shown); Local feature importance (gradient-based, perturbation-based (LIME))', actual_task:'Review sentiment analysis', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [subjective] (confidence in simulation); Understanding [objective] (forward simulation); Trust and reliance [subjective] (usage intention/willingness)', key:'nguyen2018comparing'},
  {paper:'An Evaluation of the Human-Interpretability of Explanation', paper_link:'https://arxiv.org/pdf/1902.00006.pdf', year:2019, venue:'arXiv', authors:'Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, Finale Doshi-Velez', ai_model:'Wizard of Oz', ai_assistance_element:'Prediction (multiple predictions or multi-step decision)', actual_task:'Alien medicine recommendation; Alien recipe recommendation', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error); Efficiency [objective] (time taken on the task)', ai_eval_metric:'System satisfaction and usability [subjective] (satisfaction)', key:'lage2019evaluation'},
  {paper:'To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making', paper_link:'https://scholar.harvard.edu/files/zbucinca/files/bucinca2021trust.pdf', year:2021, venue:'CSCW', authors:'Zana Buçinca, Maja Barbara Malaya, Krzysztof Z. Gajos', ai_model:'Wizard of Oz', ai_assistance_element:'Prediction (multi-class prediction, multiple predictions or multi-step decision); Model uncertainty (classification confidence (or probability)); Levels of user agency (level of machine agency)', actual_task:'Nutrition prediction', ai_task_type:'Discovery', task_eval_metric:'Task satisfaction & mental demand [subjective] (mental demand/effort)', ai_eval_metric:'Trust and reliance [objective] (over-reliance); System satisfaction and usability [subjective] (complexity)', key:'buccinca2021trust'},
  {paper:'Let Me Explain: Impact of Personal and Impersonal Explanations on Trust in Recommender Systems', paper_link:'https://dl.acm.org/doi/10.1145/3290605.3300717', year:2019, venue:'CHI', authors:'Johannes Kunkel, Tim Donkers, Lisa Michael, Catalin-Mihai Barbu, Jürgen Ziegler', ai_model:'Matrix factorization', ai_assistance_element:'Prediction (multi-class prediction); Example-based methods (Nearest neighbor or similar training instances); Interventions or workflows affecting cognitive process (source of recommendation or local explanation (human or AI))', actual_task:'Movie recommendation', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Others [subjective] (quality/soundness/completeness of explanation)', key:'kunkel2019let'},
  {paper:'Too Much, Too Little, or Just Right? Ways Explanations Impact End Users’ Mental Models', paper_link:'https://openaccess.city.ac.uk/id/eprint/6344/3/VLHCC2013.pdf', year:2013, venue:'VLHCC', authors:'Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, Weng-Keen Wong', ai_model:'K-nearest neighbors', ai_assistance_element:'Prediction (multi-class prediction); Rule-based explanations (tree-based explanation); Example-based methods (Nearest neighbor or similar training instances); Model documentation (overview of the model or algorithm); Information about training data (input features or information the model considers)', actual_task:'Music recommendation', ai_task_type:'Discovery', task_eval_metric:'Task satisfaction & mental demand [subjective] (frustration/annoyance, mental demand/effort)', ai_eval_metric:'Understanding [objective] (correctness of described model behaviors); Trust and reliance [subjective] (model confidence/acceptance, usage intention/willingness); Others [subjective] (quality/soundness/completeness of explanation)', key:'kulesza2013too'},
  {paper:'Do I trust my machine teammate?: an investigation from perception to decision', paper_link:'https://dl.acm.org/doi/abs/10.1145/3301275.3302277', year:2019, venue:'IUI', authors:'Kun Yu, Shlomo Berkovsky, Ronnie Taib, Jianlong Zhou, Fang Chen', ai_model:'', ai_assistance_element:'Prediction (binary prediction)', actual_task:'Broken glass prediction', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [objective] (forward simulation); Trust and reliance [objective] (agreement/acceptance of model suggestions)', key:'yu2019trust'},
  {paper:'Manipulating and Measuring Model Interpretability', paper_link:'https://arxiv.org/pdf/1802.07810.pdf', year:2021, venue:'CHI', authors:'Forough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, Jennifer Wortman Vaughan, Hanna Wallach', ai_model:'Linear regression', ai_assistance_element:'Prediction (continuous prediction or regression); Presentation of simple models (linear regression); Information about training data (input features or information the model considers); Interventions or workflows affecting cognitive process (user makes prediction before model, training phase)', actual_task:'Property price prediction', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (mean prediction error)', ai_eval_metric:'Understanding [subjective] (ease of understanding); Understanding [objective] (forward simulation); Trust and reliance [subjective] (self-reported trust); Trust and reliance [objective] (weight of advice, disagreement/deviation, appropriate reliance)', key:'poursabzi2018manipulating'},
  {paper:'The limits of human predictions of recidivism', paper_link:'https://advances.sciencemag.org/content/advances/6/7/eaaz0652.full.pdf', year:2020, venue:'Science Advances', authors:'Zhiyuan “Jerry” Lin, Jongbin Jung, Sharad Goel, Jennifer Skeem', ai_model:'Logistic regression', ai_assistance_element:'Model uncertainty (classification confidence (or probability))', actual_task:'', ai_task_type:'', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'', key:'jung2020limits'},
  {paper:'Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance', paper_link:'https://arxiv.org/pdf/2006.14779.pdf', year:2021, venue:'CHI', authors:'Gagan Bansal, Tongshuang Wu, Joyce Zhu, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, Daniel S. Weld', ai_model:'RoBERTa; Generalized additive models (GAMs)', ai_assistance_element:'Prediction (binary prediction, multi-class prediction, prediction with alternates); Model uncertainty (classification confidence (or probability)); Local feature importance (perturbation-based (LIME)); Natural language explanations (expert-generated rationales); Interventions or workflows affecting cognitive process (outcome feedback to user)', actual_task:'Lsat question answering; Review sentiment analysis', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions)', key:'bansal2020does'},
  {paper:'Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making', paper_link:'https://mingyin.org/paper/IUI-21/iui21.pdf', year:2021, venue:'IUI', authors:'Xinru Wang, Ming Yin', ai_model:'Logistic regression', ai_assistance_element:'Prediction (binary prediction); Example-based methods (Nearest neighbor or similar training instances); Counterfactual explanations (counterfactual examples); Global feature importance (permutation-based); Interventions or workflows affecting cognitive process (user makes prediction before model)', actual_task:'Recidivism prediction; Forest cover prediction', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [subjective] (self-reported understanding); Understanding [objective] (forward simulation, counterfactual simulation, model errors detection, comprehension quiz); Trust and reliance [subjective] (model confidence/acceptance); Trust and reliance [objective] (agreement/acceptance of model suggestions, over-reliance, under-reliance, appropriate reliance)', key:'wang2021explanations'},
  {paper:'Creative Writing with a Machine in the Loop: Case Studies on Slogans and Stories', paper_link:'https://dl.acm.org/doi/pdf/10.1145/3172944.3172983', year:2018, venue:'IUI', authors:'Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, Noah A. Smith', ai_model:'Other deep learning models', ai_assistance_element:'', actual_task:'', ai_task_type:'', task_eval_metric:'', ai_eval_metric:'', key:'clark2018creative'},
  {paper:'Algorithm Aversion: People Erroneously Avoid Algorithms after Seeing Them Err', paper_link:'https://repository.upenn.edu/cgi/viewcontent.cgi?article=1392&context=fnce_papers', year:2015, venue:'Journal of Experimental Psychology', authors:'Berkeley J. Dietvorst, Joseph P. Simmons, Cade Massey', ai_model:'', ai_assistance_element:'Prediction (continuous prediction or regression); Interventions or workflows affecting cognitive process (source of recommendation or local explanation (human or AI))', actual_task:'Sales forecast; Students\' performance forecasting', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [objective] (correctness of estimated model performance/accuracy); Trust and reliance [subjective] (self-reported trust, usage intention/willingness); Trust and reliance [objective] (choice to use the model)', key:'dietvorst2015algorithm'},
  {paper:'Human reliance on machine learning models when performance feedback is limited: Heurstics and risks', paper_link:'https://mingyin.org/paper/CHI-21/reliance.pdf', year:2021, venue:'CHI', authors:'Zhuoran Lu, Ming Yin', ai_model:'Wizard of Oz', ai_assistance_element:'Prediction (binary prediction); Interventions or workflows affecting cognitive process (user makes prediction before model)', actual_task:'Speed dating', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions, switch)', key:'lu2021human'},
  {paper:'A Human-AI Collaborative Approach for Clinical Decision Making on Rehabilitation Assessment', paper_link:'https://dl.acm.org/doi/10.1145/3411764.3445472', year:2021, venue:'CHI', authors:'Min Hun Lee, Daniel P. Siewiorek, Asim Smailagic, Alexandre Bernardino, and Sergi Bermúdez i Badia', ai_model:'Other deep learning models', ai_assistance_element:'Prediction (continuous prediction or regression, multiple predictions or multi-step decision); Model uncertainty (classification confidence (or probability))', actual_task:'Stroke rehabilitation assessment', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (agreement between labels); Task satisfaction & mental demand [subjective] (workload)', ai_eval_metric:'System satisfaction and usability [subjective] (richness/informativeness); Others [subjective] (usefulness/helpfulness of explanation)', key:'lee2021human'},
  {paper:'Data-Centric Explanations: Explaining Training Data of Machine Learning Systems to Promote Transparency', paper_link:'https://dl.acm.org/doi/10.1145/3411764.3445736', year:2021, venue:'CHI', authors:'Ariful Islam Anik,  Andrea Bunt', ai_model:'Wizard of Oz', ai_assistance_element:'No prediction shown (no prediction shown); Information about training data (full training "data explanation")', actual_task:'Bail outcomes prediction; Student admission prediction', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [subjective] (self-reported understanding); Fairness [subjective] (perceived fairness); System satisfaction and usability [subjective] (ease/comfort of use, richness/informativeness)', key:'anik2021data'},
  {paper:'Overcoming Algorithm Aversion: People Will Use Imperfect Algorithms If They Can (Even Slightly) Modify Them', paper_link:'https://pdfs.semanticscholar.org/e170/6933e9e1d15515c3a444454c15890e6d134f.pdf', year:2018, venue:'Management Science', authors:'Berkeley J. Dietvorst, Joseph P. Simmons, Cade Massey', ai_model:'Linear regression', ai_assistance_element:'Prediction (continuous prediction or regression); Levels of user agency (outcome control (after decision))', actual_task:'Students\' performance forecasting', ai_task_type:'Discovery', task_eval_metric:'Efficacy [subjective] (self-rated error/accuracy); Task satisfaction & mental demand [subjective] (satisfaction with the process, confidence in the process)', ai_eval_metric:'System satisfaction and usability [subjective] (satisfaction)', key:'dietvorst2018overcoming'},
  {paper:'How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation', paper_link:'https://arxiv.org/pdf/1802.00682.pdf', year:2018, venue:'arXiv', authors:'Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, Finale Doshi-Velez', ai_model:'Wizard of Oz', ai_assistance_element:'Prediction (multiple predictions or multi-step decision)', actual_task:'Alien medicine recommendation; Alien recipe recommendation', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (F1); Efficiency [objective] (time taken on the task)', ai_eval_metric:'System satisfaction and usability [subjective] (satisfaction)', key:'narayanan2018humans'},
  {paper:'Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making', paper_link:'https://dl.acm.org/doi/pdf/10.1145/3351095.3372852?casa_token=f4yygmCnskEAAAAA:uAKc4sEyEpUS9P9_gTjG1y88wfLfZ9ldIL2splP31TfZigkZIy_6JFDj5ivOJWj9RdPtnRpPOniX', year:2020, venue:'FAccT', authors:'Yunfeng Zhang, Q. Vera Liao, Rachel K. E. Bellamy', ai_model:'Decision trees/random forests', ai_assistance_element:'Prediction (binary prediction); Model uncertainty (classification confidence (or probability)); Local feature importance (perturbation-based SHAP); Information about training data (input features or information the model considers)', actual_task:'Income prediction', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions, switch)', key:'zhang2020effect'},
  {paper:'Human-Centric Justification of Machine Learning Predictions', paper_link:'http://www.cs.columbia.edu/nlp/papers/2017/biran_human_centric_justification_ijcai_2017.pdf', year:2017, venue:'IJCAI', authors:'Or Biran and Kathleen McKeown', ai_model:'Logistic regression', ai_assistance_element:'Prediction (binary prediction); Natural language explanations (model-generated rationales)', actual_task:'Stock price prediction', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error, F1, precision, recall)', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions); System satisfaction and usability [subjective] (satisfaction, helpfulness/support)', key:'biran2017human'},
  {paper:'Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making', paper_link:'', year:2021, venue:'arXiv', authors:'Han Liu, Vivian Lai, Chenhao Tan', ai_model:'Support-vector machines (SVMs)', ai_assistance_element:'Prediction (binary prediction, multi-class prediction); Levels of user agency (interactive explanations)', actual_task:'Recidivism prediction; Profession prediction; Deception detection', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions)', key:'liu2021understanding'},
  {paper:'Why Does My Model Fail? Contrastive Local Explanations for Retail Forecasting', paper_link:'https://arxiv.org/pdf/1908.00085.pdf', year:2020, venue:'FAT', authors:'Ana Lucic, Hinda Haned, Maarten de Rijke', ai_model:'Decision trees/random forests', ai_assistance_element:'No prediction shown (no prediction shown); Counterfactual explanations (contrastive or sensitive features)', actual_task:'Sales forecast', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [subjective] (self-reported understanding); System satisfaction and usability [subjective] (satisfaction)', key:'lucic2020does'},
  {paper:'Visualizing Uncertainty and Alternatives in Event Sequence Predictions', paper_link:'http://frankdu.org/papers/guo2019chi.pdf', year:2019, venue:'CHI', authors:'Shunan Guo, Fan Du, Sana Malik, Eunyee Koh, Sungchul Kim, Zhicheng Liu, Donghyun Kim, Hongyuan Zha, and Nan Cao.', ai_model:'Recurrent Neural Networks', ai_assistance_element:'Prediction (multi-class prediction, prediction with alternates); Model uncertainty (classification confidence (or probability))', actual_task:'Marketing email prediction', ai_task_type:'Discovery', task_eval_metric:'Efficacy [subjective] (confidence in the decisions)', ai_eval_metric:'Understanding [subjective] (ease of understanding); System satisfaction and usability [subjective] (usefulness)', key:'guo2019visualizing'},
  {paper:'The Principles and Limits of Algorithm-in-the-loop Decision Making', paper_link:'https://scholar.harvard.edu/files/bgreen/files/19-cscw.pdf', year:2019, venue:'CSCW', authors:'Ben Green, Yiling Chen', ai_model:'Decision trees/random forests', ai_assistance_element:'Prediction (continuous prediction or regression)', actual_task:'Recidivism prediction; Loan approval', ai_task_type:'Discovery', task_eval_metric:'Efficacy [subjective] (confidence in the decisions); Efficacy [objective] (Brier score)', ai_eval_metric:'Understanding [objective] (correctness of estimated model performance/accuracy); Trust and reliance [subjective] (self-reported trust); Trust and reliance [objective] (model influence (difference between conditions)); Fairness [objective] (decision bias)', key:'green2019principles'},
  {paper:'Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment', paper_link:'https://dl.acm.org/doi/10.1145/3301275.3302310', year:2019, venue:'IUI', authors:'Jonathan Dodge, Q. Vera Liao, Yunfeng Zhang, Rachel K. E. Bellamy, Casey Dugan', ai_model:'Logistic regression; Generalized additive models (GAMs)', ai_assistance_element:'Prediction (binary prediction); Local feature importance (coefficients); Example-based methods (Nearest neighbor or similar training instances); Counterfactual explanations (contrastive or sensitive features); Information about training data (aggregate statistics (e.g.\",\" demographic))', actual_task:'Recidivism prediction', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Fairness [subjective] (perceived fairness)', key:'dodge2019explaining'},
  {paper:'Exploring and Promoting Diagnostic Transparency and Explainability in Online Symptom Checkers', paper_link:'https://dl.acm.org/doi/10.1145/3411764.3445101', year:2021, venue:'CHI', authors:'Chun-Hua Tsai,  Yue You,  Xinning Gui,  Yubo Kou,  John M. Carroll', ai_model:'Decision trees/random forests', ai_assistance_element:'Prediction (binary prediction); Example-based methods (Nearest neighbor or similar training instances); Natural language explanations (model-generated rationales)', actual_task:'Covid-19 diagnosis', ai_task_type:'Discovery', task_eval_metric:'Efficacy [subjective] (self-rated error/accuracy)', ai_eval_metric:'Understanding [subjective] (perceived transparency/interpretability); Trust and reliance [subjective] (self-reported trust); System satisfaction and usability [subjective] (satisfaction, effectiveness, quality, ease/comfort of use, learning)', key:'tsai2021exploring'},
  {paper:'Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems', paper_link:'https://dl.acm.org/doi/abs/10.1145/3377325.3377498', year:2020, venue:'IUI', authors:'Zana Buçinca, Phoebe Lin, Krzysztof Z. Gajos, Elena L. Glassman', ai_model:'Wizard of Oz', ai_assistance_element:'Example-based methods (Nearest neighbor or similar training instances)', actual_task:'Nutrition prediction', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error); Task satisfaction & mental demand [subjective] (mental demand/effort)', ai_eval_metric:'Understanding [subjective] (self-reported understanding); Understanding [objective] (forward simulation); Trust and reliance [subjective] (self-reported trust); System satisfaction and usability [subjective] (helpfulness/support, appropriateness)', key:'buccinca2020proxy'},
  {paper:'Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance', paper_link:'http://erichorvitz.com/gbansal-hcomp19.pdf', year:2019, venue:'AAAI', authors:'Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S. Lasecki, Daniel S. Weld, Eric Horvitz', ai_model:'', ai_assistance_element:'No prediction shown (no prediction shown)', actual_task:'Defective object pipeline', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'Trust and reliance [objective] (choice to use the model)', key:'bansal2019beyond'},
  {paper:'Procedural Justice in Algorithmic Fairness: Leveraging Transparency and Outcome Control for Fair Algorithmic Mediation', paper_link:'https://dl.acm.org/doi/pdf/10.1145/3359284', year:2019, venue:'CSCW', authors:'Min Kyung Lee, Anuraag Jain, Hae Jin Cha, Shashank Ojha, Daniel Kusbit', ai_model:'', ai_assistance_element:'Prediction (multi-class prediction, multiple predictions or multi-step decision); Model documentation (overview of the model or algorithm); Levels of user agency (outcome control (after decision))', actual_task:'Goods division', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Fairness [subjective] (individual fairness, group fairness)', key:'lee2019procedural'},
  {paper:'Anchors: High-precision model-agnostic explanations', paper_link:'https://homes.cs.washington.edu/~marcotcr/aaai18.pdf', year:2018, venue:'AAAI', authors:'Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin', ai_model:'VQA model (hybrid LSTM and CNN)', ai_assistance_element:'Rule-based explanations (anchors)', actual_task:'Recidivism prediction; Income prediction', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [objective] (forward simulation); Trust and reliance [subjective] (self-reported trust); System satisfaction and usability [subjective] (preference/likability)', key:'ribeiro2018anchors'},
  {paper:'Mental Models of AI Agents in a Cooperative Game Setting', paper_link:'http://www.katygero.com/papers/2020_MentalModelsofAIAgents.pdf', year:2020, venue:'CHI', authors:'Katy Gero, Zahra Ashktorab, Casey Dugan, Qian Pan, James Johnson, Maria Ruiz, Sarah Miller, David Millen and Werner Geyer', ai_model:'Other deep learning models', ai_assistance_element:'No prediction shown (no prediction shown)', actual_task:'Word guessing', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (win rate); Efficiency [objective] (time taken on the task)', ai_eval_metric:'Understanding [objective] (comprehension quiz); Trust and reliance [subjective] (self-reported trust)', key:'gero2020mental'},
  {paper:'Assessing the Local Interpretability of Machine Learning Models', paper_link:'https://arxiv.org/pdf/1902.03501.pdf', year:2019, venue:'', authors:'Dylan Slack, Sorelle Friedler, Carlos Scheidegger, Chitradeep Roy', ai_model:'Logistic regression; Decision trees/random forests; Shallow (1- to 2-layer) neural networks', ai_assistance_element:'No prediction shown (no prediction shown); Counterfactual explanations (counterfactual examples); Presentation of simple models (decision trees, logistic regression, one-layer MLP)', actual_task:'Math questions', ai_task_type:'Discovery', task_eval_metric:'Efficiency [objective] (time taken on the task)', ai_eval_metric:'Understanding [objective] (forward simulation); Trust and reliance [subjective] (self-reported trust)', key:'friedler2019assessing'},
  {paper:'Anchoring Bias Affects Mental Model Formation and User Reliance in Explainable AI Systems', paper_link:'https://dl.acm.org/doi/abs/10.1145/3397481.3450639', year:2021, venue:'IUI', authors:'Mahsan Nourani, Chiradeep Roy, Jeremy E Block, Donald R Honeycutt, Tahrima Rahman, Eric Ragan, Vibhav Gogate', ai_model:'Other deep learning models', ai_assistance_element:'Prediction (binary prediction); Local feature importance (video features)', actual_task:'Activity recognition', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'Understanding [objective] (forward simulation, correctness of estimated model performance/accuracy); Trust and reliance [subjective] (perceived capability/benevolence/integrity); Others [subjective] (usefulness/helpfulness of explanation)', key:'nourani2021anchoring'},
  {paper:'A case for humans-in-the-loop: decisions in the presence of erroneous algorithmic scores', paper_link:'https://dl.acm.org/doi/pdf/10.1145/3313831.3376638', year:2020, venue:'CHI', authors:'Maria De-Arteaga, Riccardo Fogliato, Alexandra Chouldechova', ai_model:'', ai_assistance_element:'Prediction (continuous prediction or regression)', actual_task:'Child maltreatment risk prediction', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions)', key:'de2020case'},
  {paper:'A Human-Grounded Evaluation of SHAP for Alert Processing', paper_link:'https://arxiv.org/pdf/1907.03324.pdf', year:2019, venue:'KDD-XAI \'19', authors:'Hilde J.P. Weerts, Werner van Ipenburg, Mykola Pechenizkiy', ai_model:'Decision trees/random forests', ai_assistance_element:'No prediction shown (no prediction shown); Model uncertainty (classification confidence (or probability)); Local feature importance (perturbation-based SHAP)', actual_task:'Income prediction; Students\' performance forecasting', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error); Efficiency [objective] (time taken on the task); Task satisfaction & mental demand [subjective] (mental demand/effort)', ai_eval_metric:'Others [subjective] (agreement with explanation)', key:'weerts2019human'},
  {paper:'Co-design and evaluation of an intelligent decision support system for stroke rehabilitation assessment', paper_link:'https://dl.acm.org/doi/pdf/10.1145/3415227', year:2020, venue:'CSCW', authors:'Min Hun Lee,  Daniel P Siewiorek, Asim Smailagic, Alexandre Bernardino, and Sergi Berm ́udez i Badia', ai_model:'Other deep learning models; Logistic regression; Decision trees/random forests; Support-vector machines (SVMs)', ai_assistance_element:'Prediction (continuous prediction or regression, multiple predictions or multi-step decision); Model uncertainty (classification confidence (or probability))', actual_task:'Stroke rehabilitation assessment', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'System satisfaction and usability [subjective] (usefulness, preference/likability, system frustration, richness/informativeness)', key:'lee2020co'},
  {paper:'Explaining Decision-Making Algorithms through UI: Strategies to Help Non-Expert Stakeholders', paper_link:'https://dl.acm.org/doi/10.1145/3290605.3300789', year:2019, venue:'CHI', authors:'Hao-Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O\'Connell, Terrance Gray, F. Maxwell Harper, Haiyi Zhu', ai_model:'Linear regression', ai_assistance_element:'Prediction (binary prediction); Levels of user agency (interactive explanations)', actual_task:'Student admission prediction', ai_task_type:'Discovery', task_eval_metric:'Efficiency [objective] (time taken on the task)', ai_eval_metric:'Understanding [subjective] (self-reported understanding); Understanding [objective] (identifying important features, comprehension quiz); Trust and reliance [subjective] (self-reported trust)', key:'cheng2019explaining'},
  {paper:'On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection', paper_link:'https://arxiv.org/pdf/1811.07901.pdf', year:2019, venue:'FAccT', authors:'Vivian Lai, Chenhao Tan', ai_model:'Support-vector machines (SVMs)', ai_assistance_element:'Prediction (binary prediction); Example-based methods (Nearest neighbor or similar training instances); Model performance (accuracy)', actual_task:'Deception detection', ai_task_type:'Discovery', task_eval_metric:'Efficacy [subjective] (self-rated error/accuracy)', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions)', key:'lai2019human'},
  {paper:'Investigating Human + Machine Complementarity: A Case Study on Recidivism', paper_link:'https://arxiv.org/pdf/1808.09123.pdf', year:2018, venue:'', authors:'Sarah Tan, Julius Adebayo, Kori Inkpen, Ece Kamar', ai_model:'Generalized additive models (GAMs)', ai_assistance_element:'', actual_task:'', ai_task_type:'', task_eval_metric:'', ai_eval_metric:'', key:'tan2018investigating'},
  {paper:'Assessing the Impact of Automated Suggestions on Decision Maautomated heusiking: Domain Experts Mediate Model Errors but Take Less Initiative', paper_link:'https://dl.acm.org/doi/10.1145/3411764.3445522', year:2021, venue:'CHI', authors:'Ariel Levy, Monica Agrawal, Arvind Satyanarayan, David Sontag', ai_model:'', ai_assistance_element:'Prediction (multi-class prediction, multiple predictions or multi-step decision); Model uncertainty (flag low confidence); Interventions or workflows affecting cognitive process (varied model quality); Levels of user agency (level of machine agency)', actual_task:'Clinical notes annotation/medical coding', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error, recall); Efficiency [objective] (time taken on the task, total number of labels)', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions)', key:'levy2021assessing'},
  {paper:'Disparate Interactions: An Algorithm-in-the-loop Analysis of Fairness in Risk Assessments', paper_link:'https://dl.acm.org/doi/10.1145/3287560.3287563', year:2019, venue:'FAccT', authors:'Ben Green, Yiling Chen', ai_model:'Decision trees/random forests', ai_assistance_element:'Prediction (continuous prediction or regression)', actual_task:'Recidivism prediction', ai_task_type:'Discovery', task_eval_metric:'Efficacy [subjective] (confidence in the decisions); Efficacy [objective] (false positive rate, Brier score)', ai_eval_metric:'Trust and reliance [objective] (model influence (difference between conditions)); Fairness [subjective] (perceived fairness); Fairness [objective] (decision bias)', key:'green2019disparate'},
  {paper:'Impact of a deep learning assistant on the histopathologic classification of liver cancer', paper_link:'https://www.nature.com/articles/s41746-020-0232-8', year:2020, venue:'NPJ', authors:'Amirhossein Kiani, Bora Uyumazturk, Pranav Rajpurkar, Alex Wang, Rebecca Gao, Erik Jones, Yifan Yu, Curtis P. Langlotz, Robyn L. Ball, Thomas J. Montine, Brock A. Martin, Gerald J. Berry, Michael G. Ozawa, Florette K. Hazard, Ryanne A. Brown, Simon B. Chen, Mona Wood, Libby S. Allard, Lourdes Ylagan, Andrew Y. Ng & Jeanne Shen', ai_model:'Other deep learning models', ai_assistance_element:'Prediction (continuous prediction or regression, multiple predictions or multi-step decision); Model uncertainty (classification confidence (or probability)); Local feature importance (gradient-based); Levels of user agency (user direction on input data)', actual_task:'Cancer image classification', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'', key:'kiani2020impact'},
  {paper:'Human evaluation of spoken vs. visual explanations for open-domain qa', paper_link:'', year:2020, venue:'preprint', authors:'Ana Valeria Gonzalez, Gagan Bansal, Angela Fan, Robin Jia, Yashar Mehdad, and Srinivasan Iyer', ai_model:'Other deep learning models', ai_assistance_element:'Prediction (multi-class prediction)', actual_task:'Question answering', ai_task_type:'Emulation', task_eval_metric:'Efficacy [objective] (cumulative award); Efficiency [objective] (time taken on the task)', ai_eval_metric:'Trust and reliance [objective] (appropriate reliance)', key:'gonzalez2020human'},
  {paper:'What can AI do for me: Evaluating Machine Learning Interpretations in Cooperative Play', paper_link:'https://arxiv.org/pdf/1810.09648.pdf', year:2019, venue:'IUI', authors:'Shi Feng, Jordan Boyd-Graber', ai_model:'Generalized additive models (GAMs)', ai_assistance_element:'Prediction (multi-class prediction, prediction with alternates); Model uncertainty (classification confidence (or probability)); Global example-based explanations (prototypes); Levels of user agency (allowing user feedback or personalization for model)', actual_task:'Quizbowl', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'', key:'feng2018can'},
  {paper:'‘It’s Reducing a Human Being to a Percentage’; Perceptions of Justice in Algorithmic Decisions', paper_link:'https://dl.acm.org/doi/10.1145/3173574.3173951', year:2018, venue:'CHI', authors:'Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, Nigel Shadbolt', ai_model:'Wizard of Oz', ai_assistance_element:'Prediction (binary prediction); Local feature importance (Wizard of Oz); Example-based methods (Nearest neighbor or similar training instances); Counterfactual explanations (counterfactual examples); Information about training data (aggregate statistics (e.g.\",\" demographic))', actual_task:'Loan approval; Selecting overbooked airline passengers for re-routing; Determining to freeze bank accounts due to money laundering suspicion; Dynamically pricing car insurance premiums; Job promotion', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [subjective] (self-reported understanding); Fairness [subjective] (process fairness, deserved outcome, feature fairness)', key:'binns2018s'},
  {paper:'Effect of Information Presentation on Fairness Perceptions of Machine Learning Predictors', paper_link:'https://dl.acm.org/doi/10.1145/3411764.3445365', year:2021, venue:'CHI', authors:'Niels van Berkel,  Jorge Goncalves,  Daniel Russo,  Simo Hosio,  Mikael B. Skov', ai_model:'Wizard of Oz', ai_assistance_element:'No prediction shown (no prediction shown); Model documentation (model prediction distribution)', actual_task:'Recidivism prediction; Loan approval', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Fairness [subjective] (perceived fairness, feature fairness)', key:'van2021effect'},
  {paper:'Do I Look Like a Criminal? Examining how Race Presentation Impacts Human Judgement of Recidivism', paper_link:'https://dl.acm.org/doi/pdf/10.1145/3313831.3376257?casa_token=8zWy9LapVlYAAAAA:HpxFzqMYHXPVvXNo3p4Jq49ASZd22bUCfEpvPZmOqrrs-MhhRlouRrDJhHSOE--uOXvzxfoN_reX', year:2020, venue:'CHI', authors:'Keri Mallari, Kori Inkpen, Paul Johns, Sarah Tan, Divya Ramesh, Ece Kamar', ai_model:'', ai_assistance_element:'', actual_task:'', ai_task_type:'', task_eval_metric:'Efficacy [objective] (accuracy/error, false positive rate)', ai_eval_metric:'', key:'mallari2020look'},
  {paper:'The Role of Explanations on Trust and Reliance in Clinical Decision Support Systems', paper_link:'https://ieeexplore.ieee.org/document/7349687', year:2015, venue:'international conference on healthcare informatics. IEEE', authors:'Adrian Bussone, Simone Stumpf, Dympna O\'Sullivan', ai_model:'Wizard of Oz', ai_assistance_element:'Prediction (multi-class prediction); Model uncertainty (classification confidence (or probability))', actual_task:'Balance disorder diagnosis', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions, over-reliance, under-reliance)', key:'bussone2015role'},
  {paper:'The effects of example-based explanations in a machine learning interface', paper_link:'https://dl.acm.org/doi/pdf/10.1145/3301275.3302289?casa_token=uolLKdRHb_oAAAAA:UXAb3kP6kq-0daXkZ7t8coPkknKlkbmTV75uLv4dL0Co1zb6qRFtcME9y78x9M70ZST8GGsSvFSk', year:2019, venue:'IUI', authors:'Carrie J. Cai, Jonas Jongejan, Jess Holbrook', ai_model:'Other deep learning models', ai_assistance_element:'Prediction (multi-class prediction); Example-based methods (Nearest neighbor or similar training instances); Global example-based explanations (prototypes)', actual_task:'Draw-and-guess', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [subjective] (self-reported understanding); System satisfaction and usability [objective] (time spent on the application); Others [subjective] (attribution to AI versus self)', key:'cai2019effects'},
  {paper:'Human-Centered Tools for Coping with Imperfect Algorithms during Medical Decision-Making', paper_link:'https://dl.acm.org/doi/10.1145/3290605.3300234', year:2019, venue:'CHI', authors:'Carrie J. Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S. Corrado, Martin C. Stumpe, Michael Terry', ai_model:'Other deep learning models', ai_assistance_element:'No prediction shown (no prediction shown); Example-based methods (Nearest neighbor or similar training instances); Levels of user agency (interactive explanations, user direction on input data)', actual_task:'Cancer image search', ai_task_type:'Discovery', task_eval_metric:'Task satisfaction & mental demand [subjective] (workload)', ai_eval_metric:'Trust and reliance [subjective] (usage intention/willingness); System satisfaction and usability [subjective] (helpfulness/support); Others [subjective] (usefulness/helpfulness of explanation)', key:'cai2019human'},
  {paper:'“Why Should I Trust You?” Explaining the Predictions of Any Classifier', paper_link:'https://arxiv.org/abs/1602.04938', year:2016, venue:'KDD', authors:'Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin', ai_model:'Support-vector machines (SVMs)', ai_assistance_element:'', actual_task:'Religion prediction', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Trust and reliance [objective] (choice to use the model)', key:'ribeiro2016should'},
  {paper:'Understanding the Effect of Accuracy on Trust in Machine Learning Models', paper_link:'https://dl.acm.org/doi/pdf/10.1145/3290605.3300509?casa_token=jA1p99lWFdcAAAAA:ZjpEsa1CML9qI_PFIw-gFiOsRGYHGsBwYycX11sMziU_-q1caN5dUdbc_vQafdsueIrl4CANILHR', year:2019, venue:'CHI', authors:'Ming Yin, Jennifer Wortman Vaughan, Hanna Wallach', ai_model:'Other deep learning models; Support-vector machines (SVMs)', ai_assistance_element:'Prediction (binary prediction); Model performance (accuracy); Interventions or workflows affecting cognitive process (user makes prediction before model, varied model quality)', actual_task:'Speed dating', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions, switch)', key:'yin2019understanding'},
  {paper:'Hafez: an Interactive Poetry Generation System', paper_link:'https://www.aclweb.org/anthology/P17-4008.pdf', year:2017, venue:'ACL', authors:'Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, Kevin Knight', ai_model:'Other deep learning models', ai_assistance_element:'', actual_task:'', ai_task_type:'', task_eval_metric:'', ai_eval_metric:'', key:'ghazvininejad2017hafez'},
  {paper:'Leveraging Rationales to Improve Human Task Performance', paper_link:'https://arxiv.org/pdf/2002.04202.pdf', year:2020, venue:'IUI', authors:'Devleena Das and Sonia Chernova', ai_model:'', ai_assistance_element:'Prediction (multi-class prediction); Natural language explanations (model-generated rationales)', actual_task:'Chess playing', ai_task_type:'Discovery', task_eval_metric:'Efficacy [subjective] (perceived performance improvement); Efficacy [objective] (win rate, human percentile rank)', ai_eval_metric:'', key:'das2020leveraging'},
  {paper:'No Explainability without Accountability: An Empirical Study of Explanations and Feedback in Interactive ML', paper_link:'https://dl.acm.org/doi/10.1145/3313831.3376624', year:2020, venue:'CHI', authors:'Alison Smith-Renner, Ron Fan, Melissa Birchfield, Tongshuang Wu, Jordan Boyd-Graber, Dan Weld, Leah Findlater', ai_model:'Naive Bayes', ai_assistance_element:'Prediction (binary prediction)', actual_task:'Email topic classification', ai_task_type:'Discovery', task_eval_metric:'Efficiency [objective] (time taken on the task); Task satisfaction & mental demand [subjective] (frustration/annoyance)', ai_eval_metric:'Understanding [subjective] (self-reported understanding); Understanding [objective] (correctness of estimated model performance/accuracy); Trust and reliance [subjective] (self-reported trust, model confidence/acceptance, perceived accuracy); System satisfaction and usability [subjective] (system frustration); Others [subjective] (desire to provide feedback, expected model improvement)', key:'smithrenner2020'},
  {paper:'A Slow Algorithm Improves Users\' Assessments of the Algorithm\'s Accuracy', paper_link:'https://dl.acm.org/doi/abs/10.1145/3359204', year:2019, venue:'CSCW', authors:'Joon Sung Park, Rick Barber, Alex Kirlik, Karrie Karahalios', ai_model:'Wizard of Oz', ai_assistance_element:'Prediction (multi-class prediction, continuous prediction or regression); Interventions or workflows affecting cognitive process (vary system response times, varied model quality)', actual_task:'Jellybean counting', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Trust and reliance [objective] (switch)', key:'park2019slow'},
  {paper:'Explanations as Mechanisms for Supporting Algorithmic Transparency', paper_link:'https://dl.acm.org/doi/pdf/10.1145/3173574.3173677?casa_token=7DTtxjBtwTkAAAAA:hku1-9uAOfmOZU5VkyQV9lQNnSKeL8Wr8zkHPwmCP7yQkmXaeU4dGCKmpBsDo_0s0nXLYb68LKfA', year:2018, venue:'CHI', authors:'Emilee Rader, Kelley Cotter, Janghee Cho', ai_model:'', ai_assistance_element:'No prediction shown (no prediction shown); Model documentation (overview of the model or algorithm)', actual_task:'Facebook news feed prioritization', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [subjective] (perceived transparency/interpretability); Understanding [objective] (correctness of described model behaviors); Trust and reliance [subjective] (perceived capability/benevolence/integrity, usage intention/willingness); Fairness [subjective] (accountability)', key:'rader2018explanations'},
  {paper:'Insights into Human-Agent Teaming: Intelligent Agent Transparency and Uncertainty', paper_link:'https://kimberlystowers.files.wordpress.com/2017/08/stowers-et-al-2017-insights-into-human-agent-teaming.pdf', year:2017, venue:'Advances in Human Factors in Robots and Unmanned Systems ', authors:'Kimberly Stowers, Nicholas Kasdaglis, Michael Rupp, Jessie Chen, Daniel Barber, and Michael Barnes', ai_model:'', ai_assistance_element:'Prediction (multi-class prediction, prediction with alternates)', actual_task:'Military planning (monitor and direct unmanned vehicles)', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error); Efficiency [objective] (time taken on the task); Task satisfaction & mental demand [subjective] (workload)', ai_eval_metric:'System satisfaction and usability [subjective] (system usability)', key:'stowers2017insights'},
  {paper:'Tell Me More? The Effects of Mental Model Soundness on Personalizing an Intelligent Agent', paper_link:'https://openaccess.city.ac.uk/id/eprint/12412/1/', year:2012, venue:'CHI', authors:'Todd Kulesza, Simone Stumpf, Margaret Burnett, Irwin Kwan', ai_model:'', ai_assistance_element:'Prediction (multi-class prediction); Model documentation (overview of the model or algorithm); Interventions or workflows affecting cognitive process (training phase); Levels of user agency (allowing user feedback or personalization for model)', actual_task:'Music recommendation', ai_task_type:'Discovery', task_eval_metric:'Efficacy [subjective] (soundness of participants\' mental models)', ai_eval_metric:'Understanding [subjective] (confidence in understanding); Understanding [objective] (comprehension quiz); Trust and reliance [subjective] (self-reported trust); System satisfaction and usability [subjective] (preference/likability)', key:'kulesza2012tell'},
  {paper:'Taking Advice from Intelligent Systems: The Double-Edged Sword of Explanations', paper_link:'https://dl.acm.org/doi/10.1145/1943403.1943424', year:2011, venue:'IUI', authors:'Kate Erlich, Susanna Kirk, John Patterson, Jamie Rasmussen, Steven Ross, Daniel Gruen', ai_model:'', ai_assistance_element:'Prediction (multi-class prediction, prediction with alternates)', actual_task:'Cybersecurity monitoring', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'System satisfaction and usability [subjective] (helpfulness/support)', key:'ehrlich2011taking'},
  {paper:'What You See Is What You Get? The Impact of Representation Criteria on Human Bias in Hiring', paper_link:'https://arxiv.org/abs/1909.03567', year:2019, venue:'HCOMP', authors:'Andi Peng, Besmira Nushi, Emre Kiciman, Kori Inkpen, Siddharth Suri, Ece Kamar', ai_model:'Other deep learning models', ai_assistance_element:'', actual_task:'', ai_task_type:'', task_eval_metric:'', ai_eval_metric:'', key:'peng2019you'},
  {paper:'Interpretable Decision Sets: A Joint Framework for Description and Prediction', paper_link:'https://cs.stanford.edu/people/jure/pubs/interpretable-kdd16.pdf', year:2016, venue:'KDD', authors:'Himabindu Lakkaraju, Stephen H. Bach, Jure Leskovec', ai_model:'Bayesian decision lists', ai_assistance_element:'No prediction shown (no prediction shown); Rule-based explanations (decision sets)', actual_task:'Bail outcomes prediction; Medical disease diagnosis; Student dropout prediction', ai_task_type:'Discovery', task_eval_metric:'Efficiency [objective] (time taken on the task); Task satisfaction & mental demand [objective] (number of words in user feedback)', ai_eval_metric:'', key:'lakkaraju2016interpretable'},
  {paper:'Visual, textual or hybrid: the effect of user expertise on different explanations', paper_link:'https://dl.acm.org/doi/10.1145/3397481.3450662', year:2021, venue:'IUI', authors:'Maxwell Szymanski, Martijn Millecamp, Katrien Verbert', ai_model:'', ai_assistance_element:'Prediction (continuous prediction or regression)', actual_task:'News reading time prediction', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Understanding [subjective] (intuitiveness); Understanding [objective] (identifying important features); Trust and reliance [subjective] (model confidence/acceptance); Others [subjective] (quality/soundness/completeness of explanation, usefulness/helpfulness of explanation, easiness to use explanation)', key:'szymanski2021visual'},
  {paper:'\"Why is \'Chicago\' deceptive?\" Towards Building Model-Driven Tutorials for Humans', paper_link:'https://arxiv.org/pdf/2001.05871.pdf', year:2020, venue:'CHI', authors:'Vivian Lai, Han Liu, Chenhao Tan', ai_model:'BERT; Support-vector machines (SVMs)', ai_assistance_element:'Prediction (binary prediction); Local feature importance (attention); Model performance (accuracy); Global example-based explanations (model tutorial); Interventions or workflows affecting cognitive process (training phase)', actual_task:'Deception detection', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions)', key:'lai2020chicago'},
  {paper:'Evaluating Saliency Map Explanations for Convolutional Neural Networks: A User Study', paper_link:'https://dl.acm.org/doi/10.1145/3377325.3377519', year:2020, venue:'IUI', authors:'Ahmed Alqaraawi, Martin Schuessler, Philipp Weiß, Enrico Costanza, Nadia Berthouze', ai_model:'Convolution Neural Networks', ai_assistance_element:'No prediction shown (no prediction shown); Local feature importance (propagation-based (LRP), perturbation-based (LIME))', actual_task:'Image classification', ai_task_type:'Emulation', task_eval_metric:'', ai_eval_metric:'Understanding [subjective] (confidence in simulation); Understanding [objective] (forward simulation); Trust and reliance [subjective] (self-reported trust, model confidence/acceptance)', key:'alqaraawi2020evaluating'},
  {paper:'Feature-Based Explanations Don\'t Help People Detect Misclassifications of Online Toxicity', paper_link:'https://ojs.aaai.org/index.php/ICWSM/article/view/7282', year:2020, venue:'ICWSM', authors:'Samuel Carton, Qiaozhu Mei, Paul Resnick', ai_model:'Recurrent Neural Networks', ai_assistance_element:'Local feature importance (attention)', actual_task:'Toxicity classification', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error, false positive rate, false negative rate); Efficiency [objective] (time taken on the task)', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions)', key:'carton2020feature'},
  {paper:'How Do Visual Explanations Foster End Users\' Appropriate Trust in Machine Learning', paper_link:'https://dl.acm.org/doi/pdf/10.1145/3377325.3377480', year:2020, venue:'IUI', authors:'Fumeng Yang, Zhuanyi Huang, Jean Scholtz, Dustin L. Arendt', ai_model:'Support-vector machines (SVMs)', ai_assistance_element:'Prediction (multi-class prediction); Model performance (accuracy)', actual_task:'Plant classification', ai_task_type:'Discovery', task_eval_metric:'Efficiency [objective] (time taken on the task)', ai_eval_metric:'Understanding [subjective] (self-reported understanding); Trust and reliance [objective] (over-reliance, under-reliance, appropriate reliance); System satisfaction and usability [subjective] (helpfulness/support, preference/likability)', key:'yang2020visual'},
  {paper:'Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?', paper_link:'https://arxiv.org/pdf/2005.01831.pdf', year:2020, venue:'ACL', authors:'Peter Hase and Mohit Bansal', ai_model:'Other deep learning models', ai_assistance_element:'Prediction (binary prediction); Local feature importance (perturbation-based (LIME)); Rule-based explanations (anchors); Example-based methods (Nearest neighbor or similar training instances); Partial decision boundary (traversing the latent space around a data input)', actual_task:'Income prediction; Review sentiment analysis', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'Understanding [objective] (forward simulation, counterfactual simulation)', key:'hase2020evaluating'},
  {paper:'An Empirical Study on the Perceived Fairness of Realistic, Imperfect Machine Learning Models', paper_link:'https://dl.acm.org/doi/pdf/10.1145/3351095.3372831?casa_token=blTPCyHmkh4AAAAA:_yAZFHjPAafMVXHUAuZN7876UGPNGWCjL5OSC-rnZjkBvLKV64cDwpvN7Ca98E81sQaAlMYvIioo', year:2020, venue:'FAccT', authors:'Galen Harrison, Julia Hanson, Christine Jacinto, Julio Ramirez', ai_model:'', ai_assistance_element:'No prediction shown (no prediction shown); Model performance (accuracy, false positive rates); Information about training data (input features or information the model considers)', actual_task:'Bail outcomes prediction', ai_task_type:'Discovery', task_eval_metric:'', ai_eval_metric:'Fairness [subjective] (perceived fairness)', key:'harrison2020empirical'},
  {paper:'Algorithm Appreciation: People Prefer Algorithmic To Human Judgment', paper_link:'https://www.hbs.edu/ris/Publication%20Files/17-086_610956b6-7d91-4337-90cc-5bb5245316a8.pdf', year:2019, venue:'Organizational Behavior and Human Decision Processes', authors:'Jennifer M. Logg, Julia A. Minson, Don A. Moore', ai_model:'Wizard of Oz', ai_assistance_element:'Prediction (continuous prediction or regression); Interventions or workflows affecting cognitive process (source of recommendation or local explanation (human or AI))', actual_task:'Song rank order prediction; Person weight estimation; Attractiveness estimation', ai_task_type:'Discovery', task_eval_metric:'Efficacy [subjective] (confidence in the decisions)', ai_eval_metric:'Trust and reliance [objective] (weight of advice)', key:'logg2019algorithm'},
  {paper:'When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making', paper_link:'https://arxiv.org/abs/2011.06167', year:2020, venue:'arxiv', authors:'', ai_model:'Linear regression', ai_assistance_element:'Model uncertainty (uncertainty distribution (regression))', actual_task:'Apartment price prediction', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'Trust and reliance [objective] (agreement/acceptance of model suggestions, switch)', key:'mcgrath2020does'},
  {paper:'Updates in Human-AI Teams: Understanding and Addressing the Performance/Compatibility Tradeoff', paper_link:'https://ojs.aaai.org//index.php/AAAI/article/view/4087', year:2019, venue:'AAAI', authors:'Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S. Lasecki, Daniel S. Weld, Eric Horvitz', ai_model:'', ai_assistance_element:'Prediction (binary prediction); No prediction shown (no prediction shown)', actual_task:'Defective object pipeline', ai_task_type:'Discovery', task_eval_metric:'Efficacy [objective] (customized score/return)', ai_eval_metric:'', key:'bansal2019updates'},
  {paper:'Human-computer collaboration for skin cancer recognition', paper_link:'https://www.nature.com/articles/s41591-020-0942-0', year:2020, venue:'Nature Medicine', authors:'Philipp Tschandl, Christoph Rinner, Zoe Apalla, Giuseppe Argenziano, Noel Codella, Allan Halpern, Monika Janda, Aimilios Lallas, Caterina Longo, Josep Malvehy, John Paoli, Susana Puig, Cliff Rosendahl, H. Peter Soyer, Iris Zalaudek, Harald Kittler', ai_model:'Convolutional Neural Networks (CNN)', ai_assistance_element:'Prediction (multiclass prediction); Prediction (binary prediction); Content-Based Image Retrieval (CBIR)', actual_task:'Skin cancer recognition', ai_task_type:'Recognition', task_eval_metric:'Efficacy [objective] (accuracy/error)', ai_eval_metric:'', key:'tschandl2020human'},
  {paper:'Deepfake detection by human crowds, machines, and machine-informed crowds', paper_link:'https://www.pnas.org/content/119/1/e2110013119', year:2022, venue:'Proceedings of the National Academy of Science', authors:'Matthew Groh, Ziv Epstein, Chaz Firestone, Rosalind Picard', ai_model:'Convolutional Neural Networks (CNN)', ai_assistance_element:'Prediction (binary prediction)', actual_task:'Deepfake detection', ai_task_type:'Recognition', task_eval_metric:'Efficacy [objective] (accuracy/error); Efficacy [subjective] (confidence in the decisions)', ai_eval_metric:'', key:'groh2022deepfake'}


]

export default data;
